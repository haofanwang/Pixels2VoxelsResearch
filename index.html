<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pixels2Voxels Research (P2VR)</title>

    <style>
        .box {
            border-radius: 10px;
            margin: 0 auto;
            padding-left: 60px;
            padding-right: 60px;
            width: 760px;
            background-color: white;
            box-shadow: black 0px 0px 32px;
        }

        .bgb {
            background-color: rgb(240, 240, 240);
            padding-left: 60px;
            padding-right: 10px;
            padding-top: 5px;
            padding-bottom: 5px;
            margin-top: 40px;
            margin-left: -45pt;
            margin-right: -45pt;
            font-weight: bold;
        }

        .item {
            margin-top: 20px;
        }

        .left-img {
            border-radius: 10px;
            display: inline-block;
            vertical-align: top;
            width: 35%;
            margin-right: 4%;
        }

        .right-text {
            display: inline-block;
            width: 60%;
        }

        .dot {
            height: 14px;
            width: 14px;
            background-color: #bbb;
            border-radius: 50%;
            display: inline-block;
            margin-right: 8px;
        }
    </style>

</head>

<body style="background-color:rgb(48, 48, 48);">
    </br></br></br>
    <div class="box">
        <div class="block">
            <img src="imgs/logo.png"
                style="width: 880px"  /></br>
                Pixels2Voxels Research (P2VR) is a non-commercial and voluntary research group, established in February 2023, not affiliated with any official institution, dedicated to promoting extensive academic research cooperation.
                The team currently has 7 senior researchers and 10 researchers. Our research mainly includes multimodal and 3D virtual digital human. More specifically, the multimodal direction includes multimodal representation learning, audio-visual localization, commonsense reasoning, and the digital human direction includes controllable image/video generation, 3D avatar reconstruction and generation, conditional human motion generation, human-aware 3D scene generation.
                Our main target is to publish influential academic papers in the top conferences such as CVPR/ICCV/ECCV/SIGGRAPH. To join us, you have to be self-motivated and have published at least one related paper on the top tier conferences.
        </div>

        <div class="tex">
            <div class="bgb">News</div>
            <p>
                2023.02 - Our teammates got 5 papers accepted to CVPR 2023.</br>
                2023.02 - Pixels2Voxels Research is founded for spontaneous research cooperation.</br>
            </p>
        </div>

        <div class="tex">
            <div class="bgb">Member</div>
            <p>
            <li>
                Digital Avatar:
                <a href="https://haofanwang.github.io/">Haofan Wang</a>, <a href="https://xyyhw.top/">Hongwei Yi</a>, <a href="https://keras.me/">Qiongjie Cui</a>,
                <a href="http://wangjingbo.top/">Jingbo Wang</a>, <a href="https://yzhq97.github.io/">Zhuoqian Yang</a>, <a href="https://dingpx.github.io/">Pengxiang Ding</a>,
                <a href="https://yzmblog.github.io/">Zhengming Yu</a>, <a href="https://jinluzhang.site/">Jinlu Zhang</a>, <a href="https://scholar.google.com/citations?user=SO-JS9EAAAAJ&hl=zh-CN">Jiale Xu</a>,
                <a href="https://antonioo-c.github.io/">Antong Chen</a>, <a href="https://scholar.google.com.hk/citations?user=MzGqlqIAAAAJ&hl=en">Yangyi Huang</a>, Tingting Liao, Liang Pan
            </li>

            <li>
                Multi-Modality:
                <a href="https://scholar.google.com/citations?user=6aYncPAAAAAJ&hl=en">Shentong Mo</a>, <a href="https://yulu.net.cn/">Yu Lu</a>, 
                <a href="https://github.com/PeterQiu0516">Changyuan Qiu</a>, <a href="https://github.com/ThreeSR">Rui Sun</a>
            </li>
            </p>
        </div>

        <div class="tex">
            <div class="bgb">Contact Us</div>
            <ul style="padding-left: 24px;">
                <li style="padding-bottom: 10px;">To contact Pixels2Voxels Research for academic/commercial
                    collaboration, please send e-mail to haofanwang.ai@gmail.com with
                    "Interested in Pixels2Voxels Research" in your e-mail title, and breifly introduce your research background.</li>
            </ul>
        </div>

    </div>
    </br></br></br>
</body>
